# -*- coding: utf-8 -*-
"""santa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U69wF81lDU9WOVW3UvYot08mCX9t_RgE
"""

!gdown 1Hlqa8bH_OR0AvEv4Mj_M-d2-WKTSKtJJ

!unzip -qq archive.zip

import os
from glob import glob
import cv2
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torchvision import models
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder

import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import seaborn as sns
import pandas as pd

DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

tfm = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(degrees=20),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

train = ImageFolder("/content/is that santa/train" , transform = tfm)
test = ImageFolder("/content/is that santa/test" , transform = tfm)

len_train = len(train)
len_test = len(test)

print(len_train)
print(len_test)

#Mapping the index
train.class_to_idx

train_loader = DataLoader(train,
                          batch_size = 30,
                          drop_last = False,
                          shuffle = True)

test_loader = DataLoader(test,
                        batch_size = 30,
                        drop_last = False,
                        shuffle = False)

class santa(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.conv1 = nn.Conv2d(3,10,kernel_size = 3, stride=1,padding=1)
        self.norm1 = nn.BatchNorm2d(10)
        self.conv2 = nn.Conv2d(10,20,kernel_size = 3,stride = 1,padding = 1)
        self.norm2 = nn.BatchNorm2d(20)
        self.conv3 = nn.Conv2d(20,40,kernel_size = 3,stride = 1,padding = 1)
        self.norm3 = nn.BatchNorm2d(40)
        self.pool = nn.MaxPool2d(2, 2)
        self.act_fn = nn.ReLU()

        self.linear1 = nn.Linear(40*16*16, 100)
        self.linear2 = nn.Linear(100, num_classes)
        self.drop = nn.Dropout(p=0.2)

    def forward(self, x):
        x = self.pool(F.relu(self.norm1(self.conv1(x))))
        x = self.pool(F.relu(self.norm2(self.conv2(x))))
        x = self.pool(F.relu(self.norm3(self.conv3(x))))
        x = x.view(-1, 40*16*16)
        x = self.drop(x)
        x = F.relu(self.linear1(x))
        x = self.drop(x)
        x = self.linear2(x)

        return x

model = santa()
model

model.to(DEVICE)
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0001)
EPOCHS = 10

def train_fn(train_loader, model, loss_func, optimizer):
    losses = []
    model.train()

    progress = tqdm(train_loader, total=len(train_loader))
    for i,(imgs, labels) in enumerate(progress):
        imgs = imgs.to(DEVICE)
        labels = labels.to(DEVICE)

        y_preds = model(imgs)
        loss = loss_func(y_preds, labels) #loss function created

        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero
        loss.backward()               # Back Propagation
        optimizer.step()              # Updating the parameters

        losses.append(loss.item())

    return np.mean(losses)

def test_fn(test_loader,model,loss_func,optimizer):
    losses = []
    model.eval()

    progress = tqdm(test_loader, total=len(test_loader))
    for i,(imgs, labels) in enumerate(progress):
        imgs = imgs.to(DEVICE)
        labels = labels.to(DEVICE)

        with torch.no_grad():
            y_pred = model(imgs)
            loss = loss_func(y_pred,labels)

        losses.append(loss.item())


    return np.mean(losses)

train_losses = []
test_losses = []

best_dict = None
best_loss = np.inf

for ep in range(EPOCHS):
    print('='*5 + f" Epoch {ep+1} " + '='*5)

    tr_loss = train_fn(train_loader, model, loss_func, optimizer)
    ts_loss = test_fn(test_loader, model, loss_func, optimizer)

    # EARLY STOPPING
    if ts_loss < best_loss:
        best_loss = ts_loss
        best_dict = model.state_dict()

    train_losses.append(tr_loss)
    test_losses.append(ts_loss)

    print(f"Epoch {ep} - Train Loss {tr_loss:.4f} - test Loss {ts_loss:.4f}\n")

plt.plot(train_losses)

plt.plot(test_losses)

model.load_state_dict(best_dict)

test_preds = []
test_trues = []

progress = tqdm(test_loader, total=len(test_loader))
model.eval()

for i, (imgs, labels) in enumerate(progress):
    imgs = imgs.to(DEVICE)
    labels = labels.to(DEVICE)

    with torch.no_grad():
        y_preds = model(imgs)

    pred_labels = np.argmax(y_preds.cpu().detach().numpy(), axis=1)
    test_preds.extend(pred_labels.tolist())
    test_trues.extend(labels.cpu().detach().numpy().tolist())

metrics.accuracy_score(test_preds, test_trues)

